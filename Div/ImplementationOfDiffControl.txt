Implementation of Diff-Control in SDP

Goal: Inject past action history into the denoising model via a ControlNet-like module,
making the policy stateful and able to "remember" the goal direction even after it disappears.

Skal legge til en ControlNet branch som er en parallell nn module som tar inn past actions og encoder det til til en feature map for
så å legge til de featurene i hoved Unet modellen flere steder under denoising. 


1. Lagd copier av tedi_unet_lowdim_policy og conditional_unet1d_tedi og byttet til TEDiState navn.
2. Endret conditional_unet1d_tedi_state.
- This only adds functionality — nothing breaks in your original model.
- If past_action_cond is None, the model behaves exactly like before.
- You can now condition your streaming diffusion model on past actions.

3. Endret tedi_state_unet_lowdim_policy.
- Added past_action_horizon to __init__
- Reset past_action_buffer in reset_buffer
- Add a helper to get past actions
- Store executed actions into the buffer
- Pass past actions into model call
- Do the same in compute_loss()

I have now:
- Added a past-action conditioning buffer,
- Fed it into a ControlNet-style model (StatefulConditionalUnet1D),
- Preserved all original behavior when past_action_cond is None.

4. Kopiert train_tedi_unet_ddim_pusht_memory_lowdim_workspace.yaml og lagd ny.
Endret policy og model target til de nye filene jeg har lagd. Lagt til action horizon og ført til model og policy.

Yes, Your Implementation Covers All the Necessary Components for Diff Control:
After reviewing your recent implementations and the configuration file, you have successfully:

Updated the StatefulConditionalUnet1D model:
- Integrated the past_action_cond input.
- Implemented the past action encoder.
- Injected the past action feature in the forward pass.

Updated the Policy Class:
- Added the past_action_horizon and past_action_buffer.
- Modified the predict_action method to condition on the past actions.
- Adjusted the initialize_buffer and conditional_sample methods to handle past action conditioning.

Updated the Training Configuration:
- Added past_action_visible and past_action_horizon.
- Adjusted the StatefulConditionalUnet1D configuration to accept the past_action_dim.

Lager nytt env:
- Added a buffer to store past actions
- Updated the step function to store past actions.

Nytt demo script:
- Kopiert det gamle og byttet navn.
- Lagt til past action buffer
- Include past action in data collection
- Update past action buffer etter hver gang vi tar et step
- Added a reset of the past action buffer before retry break.

Ny dataset.py fil:
- Update the ReplayBuffer initialization to include past_actions.
- Extract and handle the past_actions data in the _sample_to_data method.
- Include past_actions in the data dictionary returned by the __getitem__ method.



Past action visible and past action horizon i config:
past_action_visible: This is a boolean flag that controls whether the model and scheduler should consider past actions as conditioning inputs.
It is used to enable or disable the past action conditioning mechanism.
past_action_horizon: This is an integer value that specifies the number of past actions to be considered as context for the current denoising step.
It defines the length of the window of past actions.

In the model init:
If past_action_visible is True, past_action_dim is set to the action_dim.
If past_action_visible is False, past_action_dim is set to None.
This configuration is used to define the size of the past action encoder in the StatefulConditionalUnet1D model.

In the model forward pass: 
If past_action_visible is True, the get_past_action_cond() method will return a tensor of past actions based on the past_action_horizon.
If False, the method will return None, and the model will proceed without past action conditioning.

In the scheduler:
If past_action_visible is None, the scheduler will ignore past actions.
If past_action_visible is a tensor, it will be injected into the denoising process as control information.

Endret ddim og ddpm scheduler:
- Lagt til past_action_visible i step functionen som en optional.
- La til past action conditioning.

"The addition of past_action_visible is effectively a residual connection.
This is crucial because it allows the model to adjust its prediction based on prior actions without completely overriding the model's output."


OBS: Bruker bare past_action_weight i scheduler. Skal jeg ha det i modellen???





Diff-Control steps:
SDP struggles with memory because it’s a stateless policy that relies only on the current observation, making it ineffective when goal information disappears (as in your Push-T environment).

Diff-Control introduces memory by adding a ControlNet-based transition model that conditions the policy on previous actions, effectively acting as a learned temporal prior.

This transition model allows the policy to maintain goal-directed behavior even after the goal is no longer visible, by continuing motion consistent with past actions.

ControlNet is compatible with SDP since both use U-Net-style diffusion models, and the additional conditioning from previous actions can be injected efficiently into the denoising process.

The implementation involves adding a lightweight ControlNet branch to the SDP network, which takes in recent action history and modulates the output of the base diffusion model.

Training is done in two stages: first, train SDP as usual, then freeze it and train the ControlNet branch to inject temporal consistency using demonstration data.

This preserves SDP’s low-latency performance, because the added computation is minor and no extra diffusion steps are introduced.

In your task, this means the robot can remember where the goal was and continue pushing the block in the right direction, even when the goal disappears.

The key requirement is that the robot’s previous actions contain meaningful directional intent, which the ControlNet can learn to carry forward.

Overall, this approach would add memory and statefulness to SDP, likely solving the issue you observed and enabling better performance in partially observable tasks like Push-T.



Spurte chat om det kommer til å bli vanskelig:

You don’t need to re-architect SDP completely — you're augmenting it, not replacing it.

ControlNet is modular: you can clone part of the existing SDP U-Net (the encoder), attach it in parallel, and inject its features via simple additions or residuals.

Two-stage training simplifies things: you freeze the base SDP policy and only train the new ControlNet branch, reducing training instability.

You can use existing open-source code as a template — both SDP and Diff-Control are publicly available and well-documented.

Hva som krever innstas:
You’ll need to design and integrate a ControlNet-style encoder for previous actions (e.g., past 5–10 actions, using 1D convs or MLPs).

You must inject those features into the main U-Net at the right places — typically after downsampling stages, using additive or concat-based conditioning.

Training requires good supervision — if your Push-T demos don’t include examples with goal occlusion, you might need to augment them.

You’ll need to validate and tune the balance between the observation model and the ControlNet prior — too strong a prior could reduce reactivity.